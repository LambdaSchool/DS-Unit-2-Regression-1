{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "_Lambda School Data Science — Regression 1_\n\n# Understanding Ordinary Least Squares\n\n#### Objectives\n- understand how ordinary least squares regression minimizes the sum of squared errors\n- understand how linear algebra can solve ordinary least squares regression\n- get and interpret coefficients of a linear model\n- visualize a line of best fit in 2D, and hyperplane in 3D\n- use regression metrics: MSE, RMSE, MAE, R^2\n\n#### Extra Links\n- [Statistics 101: Simple Linear Regression](https://www.youtube.com/watch?v=ZkjP5RJLQF4) (20 minute video)\n- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), Chapter 3.1, Simple Linear Regression, & 3.2, Multiple Linear Regression\n- Priceonomics, [The Discovery of Statistical Regression](https://priceonomics.com/the-discovery-of-statistical-regression/)\n- Priceonomics, [Why the Father of Modern Statistics Didn’t Believe Smoking Caused Cancer](https://priceonomics.com/why-the-father-of-modern-statistics-didnt-believe/)\n- Harvard Business Review, [When to Act on a Correlation, and When Not To](https://hbr.org/2014/03/when-to-act-on-a-correlation-and-when-not-to)\n- [xkcd 552: Correlation](https://www.explainxkcd.com/wiki/index.php/552:_Correlation)\n- [xkcd 1725: Linear Regression](https://www.explainxkcd.com/wiki/index.php/1725:_Linear_Regression)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## What is Linear Regression?\n\n[Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) is a statistical model that seeks to describe the relationship between some y variable and one or more x variables. \n\n<img alt=\"Linear Regression\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png\" width=\"600\">\n\nIn the simplest case, linear regression seeks to fit a straight line through a cloud of points. This line is referred to as the \"regression line\" or \"line of best fit.\" This line tries to summarize the relationship between our X and Y in a way that enables us to use the equation for that line to make predictions.\n\n### Synonyms for \"y variable\" \n- Dependent Variable\n- Response Variable\n- Outcome Variable \n- Predicted Variable\n- Measured Variable\n- Explained Variable\n- Label\n- Target\n\n### Synonyms for \"x variable\"\n- Independent Variable\n- Explanatory Variable\n- Regressor\n- Covariate\n- Feature"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Simple Linear Regresion\n\n#### Making Predictions\n\nSay that we were trying to create a model that captured the relationship between temperature outside and ice cream sales. In Machine Learning our goal is often different that of other flavors of Linear Regression Analysis, because we're trying to fit a model to this data with the intention of making **predictions** on new data (in the future) that we don't have yet.\n\n#### What are we trying to predict?\n\nSo if we had measured ice cream sales and the temprature outside on 11 different days, at the end of our modeling **what would be the thing that we would want to predict? - Ice Cream Sales or Temperature?**\n\nWe would probably want to be measuring temperature with the intention of using that to **forecast** ice cream sales. If we were able to successfully forecast ice cream sales from temperature, this might help us know beforehand how much ice cream to make or how many cones to buy or on which days to open our store, etc. Being able to make predictions accurately has a lot of business implications. This is why making accurate predictions is so valuable (And in large part is why data scientists are paid so well).\n\n#### y variable intuition\n\nWe want the thing that we're trying to predict to serve as our **y** variable. This is why it's sometimes called the \"predicted variable.\" We call it the \"dependent\" variable because our prediction for how much ice cream we're going to sell \"depends\" on the temperature outside. \n\n#### x variable intuition\n\nAll other variables that we use to predict our y variable (we're going to start off just using one) we call our **x** variables. These are called our \"independent\" variables because they don't *depend* on y, they \"explain\" y. Hence they are also referred to as our \"explanatory\" variables."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Example, reprised: Predict presidential election voting\n\n#### Douglas Hibbs, [Background Information on the ‘Bread and Peace’ Model of Voting in Postwar US Presidential Elections](https://douglas-hibbs.com/background-information-on-bread-and-peace-voting-in-us-presidential-elections/)\n\n> Aggregate two-party vote shares going to candidates of the party holding the presidency during the postwar era are well explained by just two fundamental determinants:\n\n> (1) Positively by weighted-average growth of per capita real disposable personal income over the term.  \n> (2) Negatively by cumulative US military fatalities (scaled to population) owing to unprovoked, hostile deployments of American armed forces in foreign wars.\n\n#### Data sources\n- 1952-2012: Douglas Hibbs, [2014 lecture at Deakin University Melbourne](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 40\n- 2016, Vote Share: [The American Presidency Project](https://www.presidency.ucsb.edu/statistics/elections)\n- 2016, Recent Growth in Personal Incomes: [The 2016 election economy: the \"Bread and Peace\" model final forecast](https://angrybearblog.com/2016/11/the-2016-election-economy-the-bread-and-peace-model-final-forecast.html)\n- 2016, US Military Fatalities: Assumption that Afghanistan War fatalities in 2012-16 occured at the same rate as 2008-12"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\ncolumns = ['Year','Incumbent Party Candidate','Other Candidate','Incumbent Party Vote Share']\n\ndata = [[1952,\"Stevenson\",\"Eisenhower\",44.6],\n        [1956,\"Eisenhower\",\"Stevenson\",57.76],\n        [1960,\"Nixon\",\"Kennedy\",49.91],\n        [1964,\"Johnson\",\"Goldwater\",61.34],\n        [1968,\"Humphrey\",\"Nixon\",49.60],\n        [1972,\"Nixon\",\"McGovern\",61.79],\n        [1976,\"Ford\",\"Carter\",48.95],\n        [1980,\"Carter\",\"Reagan\",44.70],\n        [1984,\"Reagan\",\"Mondale\",59.17],\n        [1988,\"Bush, Sr.\",\"Dukakis\",53.94],\n        [1992,\"Bush, Sr.\",\"Clinton\",46.55],\n        [1996,\"Clinton\",\"Dole\",54.74],\n        [2000,\"Gore\",\"Bush, Jr.\",50.27],\n        [2004,\"Bush, Jr.\",\"Kerry\",51.24],\n        [2008,\"McCain\",\"Obama\",46.32],\n        [2012,\"Obama\",\"Romney\",52.00], \n        [2016,\"Clinton\",\"Trump\",48.2]]\n        \nvotes = pd.DataFrame(data=data, columns=columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "columns = ['Year','Average Recent Growth in Personal Incomes']\n\ndata = [[1952,2.40],\n        [1956,2.89],\n        [1960, .85],\n        [1964,4.21],\n        [1968,3.02],\n        [1972,3.62],\n        [1976,1.08],\n        [1980,-.39],\n        [1984,3.86],\n        [1988,2.27],\n        [1992, .38],\n        [1996,1.04],\n        [2000,2.36],\n        [2004,1.72],\n        [2008, .10],\n        [2012, .95], \n        [2016, .10]]\n        \ngrowth = pd.DataFrame(data=data, columns=columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\"\"\"\nFatalities denotes the cumulative number of American military\nfatalities per millions of US population the in Korea, Vietnam,\nIraq and Afghanistan wars during the presidential terms\npreceding the 1952, 1964, 1968, 1976 and 2004, 2008 and\n2012 elections.\n\nhttp://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf\n\"\"\"\n\ncolumns = ['Year','US Military Fatalities per Million']\n\ndata = [[1952,190],\n        [1956,  0],\n        [1960,  0],\n        [1964,  1],\n        [1968,146],\n        [1972,  0],\n        [1976,  2],\n        [1980,  0],\n        [1984,  0],\n        [1988,  0],\n        [1992,  0],\n        [1996,  0],\n        [2000,  0],\n        [2004,  4],\n        [2008, 14],\n        [2012,  5], \n        [2016,  5]]\n        \ndeaths = pd.DataFrame(data=data, columns=columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Acquire new features"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Shark attack data source\n- https://www.sharkattackfile.net/incidentlog.htm (Download the Excel file manually, because web crawlers are blocked)\n\n#### Economic data sources\n- Unemployment: https://fred.stlouisfed.org/series/UNRATE\n- GDP: https://fred.stlouisfed.org/series/GDPC1\n- GDP change: https://fred.stlouisfed.org/series/A191RL1Q225SBEA"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# from google.colab import files\n# files.upload()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sharks = pd.read_excel('GSAF5.xls')[['Year', 'Country']]\nsharks = (sharks\n          .where(sharks.Country=='USA')\n          .groupby('Year')\n          .count()\n          .reset_index()\n          .rename(columns={'Country': 'Shark Attacks'}))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "url = 'https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1168&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=UNRATE&scale=left&cosd=1948-01-01&coed=2019-04-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2009-06-01&line_index=1&transformation=lin&vintage_date=2019-05-30&revision_date=2019-05-30&nd=1948-01-01'\nunemployment = pd.read_csv(url, parse_dates=['DATE'])\n\n# Annual average unemployment, only using the first 10 months of the year\n# (because presidential elections are in November)\nunemployment = (unemployment\n                .where(unemployment.DATE.dt.month <= 10)\n                .set_index('DATE')\n                .resample('A')\n                .mean()\n                .reset_index()\n                .rename(columns={'DATE': 'Year', 'UNRATE': 'Unemployment Rate'}))\n\nunemployment['Year'] = unemployment['Year'].dt.year",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "url = 'https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1168&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=GDPC1&scale=left&cosd=1947-01-01&coed=2019-01-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2009-06-01&line_index=1&transformation=lin&vintage_date=2019-05-30&revision_date=2019-05-30&nd=1947-01-01'\n\ngdp = pd.read_csv(url, parse_dates=['DATE'])\ngdp = (gdp\n       .where(gdp.DATE.dt.month==7)\n       .rename(columns={'DATE': 'Year', 'GDPC1': 'GDP Q3'}))\n\ngdp['Year'] = gdp['Year'].dt.year",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "url = 'https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1168&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=A191RL1Q225SBEA&scale=left&cosd=1947-04-01&coed=2019-01-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2009-06-01&line_index=1&transformation=lin&vintage_date=2019-05-30&revision_date=2019-05-30&nd=1947-04-01'\n\ngdp_change = pd.read_csv(url, parse_dates=['DATE'])\ngdp_change = (gdp_change\n              .where(gdp_change.DATE.dt.month==7)\n              .rename(columns={'DATE': 'Year', 'A191RL1Q225SBEA': 'GDP Change Q3'}))\n\ngdp_change['Year'] = gdp_change['Year'].dt.year",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Merge data"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = (votes\n      .merge(growth)\n      .merge(deaths)\n      .merge(sharks)\n      .merge(unemployment)\n      .merge(gdp)\n      .merge(gdp_change))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Engineer new feature"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# True Incumbent =\n# The Incumbent Party Candidate this election is the same as the Incumbent Party Candidate 1 election ago,\n# OR, the Incumbent Party Candidate this election is the same as the Other Candidate 1 election ago\ndf['True Incumbent'] = ((df['Incumbent Party Candidate'] == df['Incumbent Party Candidate'].shift(1)) | \n                        (df['Incumbent Party Candidate'] == df['Other Candidate'].shift(1)))\n\n# Change the data type of this feature from boolean (True/False) to integer (1/0)\ndf['True Incumbent'] = df['True Incumbent'].astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.groupby('True Incumbent')['Incumbent Party Vote Share'].mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Plot univariate correlations\n[Seaborn tutorial: Visualizing linear relationships](https://seaborn.pydata.org/tutorial/regression.html)"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#TODO",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can see from the scatterplot that these data points seem to follow a somewhat linear relationship for the \"Average Recent Growth in Personal Incomes\" feature. This means that we could probably summarize their relationship well by fitting a line of best fit to these points. Lets do it.\n\n\n## The Equation for a Line\n\nA common equation for a line is:\n\n\\begin{align}\ny = mx + b\n\\end{align}\n\nWhere $m$ is the slope of our line and $b$ is the y-intercept. \n\nIf we want to plot a line through our cloud of points we figure out what these two values should be. Linear Regression seeks to **estimate** the slope and intercept values that describe a line that best fits the data points."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## The Anatomy of Linear Regression\n\n- Intercept: The $b$ value in our line equation $y=mx+b$\n- Slope: The $m$ value in our line equation $y=mx+b$. These two values together define our regression line.\n\n![Slope and Intercept](http://www.ryanleeallred.com/wp-content/uploads/2018/08/linear-regression-diagram.png)\n\n- $\\hat{y}$ : A prediction\n- Line of Best Fit (Regression Line)\n- Predicted (fitted) Values: Points on our regression line\n- Observed Values: Points from our dataset\n- Error: The distance between predicted and observed values.\n\n![Residual Error](http://www.ryanleeallred.com/wp-content/uploads/2018/08/residual-or-error.gif)\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## R Squared:  $R^2$\n\nOne final attribute of linear regressions that we're going to talk about today is a measure of goodness of fit known as $R^2$ or R-squared. $R^2$ is a statistical measure of how close the data are fitted to our regression line. A helpful interpretation for the $R^2$ is the percentage of the dependent variable that is explained by the model.\n\nIn other words, the $R^2$ is the percentage of y that is explained by the x variables included in the model. For this reason the $R^2$ is also known as the \"coefficient of determination,\" because it explains how much of y is explained (or determined) by our x varaibles. We won't go into the calculation of $R^2$ today, just know that a higher $R^2$ percentage is nearly always better and indicates a model that fits the data more closely. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Residual Error \n\nThe residual error is the distance between points in our dataset and our regression line."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def regression_residuals(df, feature, target, m, b):\n    x = df[feature]\n    y = df[target]\n    y_pred = m*x + b\n    \n    plt.scatter(x, y, label='y_true')\n    plt.plot(x, y_pred, label='y_pred')\n    plt.legend()\n    \n    # Plot residual errors\n    for x, y1, y2 in zip(x, y, y_pred):\n        plt.plot((x, x), (y1, y2), color='grey')\n        \n    mae = mean_absolute_error(y, y_pred) \n    r2 = r2_score(y, y_pred)\n    print('Mean Absolute Error:', mae)\n    print('R^2:', r2)\n        \nregression_residuals(df, feature='Average Recent Growth in Personal Incomes', \n                     target='Incumbent Party Vote Share', m=3, b=46)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from ipywidgets import interact, fixed\n\ninteract(regression_residuals, \n         df=fixed(df), \n         feature=fixed('Average Recent Growth in Personal Incomes'), \n         target=fixed('Incumbent Party Vote Share'), \n         m=(-10,10,0.5), \n         b=(40,60,0.5));",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## More Formal Notation\n\n<img src=\"http://www.ryanleeallred.com/wp-content/uploads/2018/08/simple-regression-formula.png\" width=\"600\">\n\nWe have talked about a line of regression being represented like a regular line $y=mx+b$ but as we get to more complicated versions we're going to need to extend this equation. So lets establish the proper terminology.\n\n**X** - Independent Variable, predictor variable, explanatory variable, regressor, covariate\n\n**Y** - Response variable, predicted variable, measured vairable, explained variable, outcome variable\n\n$\\beta_0$ - \"Beta Naught\" or \"Beta Zero\", the intercept value. This is how much of y would exist if X were zero. This is sometimes represented by the letter \"a\" but I hate that. So it's \"Beta 0\" during my lecture.\n\n$\\beta_1$ - \"Beta One\" The primary coefficient of interest. This values is the slope of the line that is estimated by \"minimizing the sum of the squared errors/residuals\" - We'll get to that. \n\n$\\epsilon$ - \"Epsilon\" The \"error term\", random noise, things outside of our model that affect y."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Minimizing the Sum of the Squared Error\n\nThe most common method of estimating our $\\beta$ parameters  is what's known as \"Ordinary Least Squares\" (OLS). (There are different methods of arriving at a line of best fit). OLS estimates the parameters that minimize the squared distance between each point in our dataset and our line of best fit. \n\n\\begin{align}\nSSE = \\sum(y_i - \\hat{y})^2\n\\end{align}\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from matplotlib.patches import Rectangle\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\ndef regression_squared_errors(df, feature, target, m, b):\n    x = df[feature]\n    y = df[target]\n    y_pred = m*x + b\n    \n    fig = plt.figure(figsize=(7,7))\n    ax = plt.axes()\n    ax.scatter(x, y, label='y_true')\n    ax.plot(x, y_pred, label='y_pred')\n    ax.legend()\n    \n    # Plot square errors\n    xmin, xmax = ax.get_xlim()\n    ymin, ymax = ax.get_ylim()\n    scale = (xmax-xmin)/(ymax-ymin)\n    for x, y1, y2 in zip(x, y, y_pred):\n        bottom_left = (x, min(y1, y2))\n        height = abs(y1 - y2)\n        width = height * scale\n        ax.add_patch(Rectangle(xy=bottom_left, width=width, height=height, alpha=0.1))\n        \n    mse = mean_squared_error(y, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n    print('Mean Squared Error:', mse)\n    print('Root Mean Squared Error:', rmse)\n    print('Mean Absolute Error:', mae)\n    print('R^2:', r2)\n    \nregression_squared_errors(df, feature='Average Recent Growth in Personal Incomes', \n           target='Incumbent Party Vote Share', m=3, b=46)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "interact(regression_squared_errors, \n         df=fixed(df), \n         feature=fixed('Average Recent Growth in Personal Incomes'), \n         target=fixed('Incumbent Party Vote Share'), \n         m=(-10,10,0.5), \n         b=(40,60,0.5));",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Hypotheses"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "b = 46\nms = np.arange(-10,10,0.5)\nsses = []\n\nfor m in ms:\n    predictions = m * df[feature] + b\n    errors = predictions - df[target]\n    square_errors = errors ** 2\n    sse = square_errors.sum()\n    sses.append(sse)\n    \nhypotheses = pd.DataFrame({'Slope': ms})\nhypotheses['Intercept'] = b\nhypotheses['Sum of Square Errors'] = sses\n\nhypotheses",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "hypotheses.plot(x='Slope', y='Sum of Square Errors', \n                title=f'Intercept={b}');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Scikit-learn\n\n#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n\n> Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n\n> 1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn. \n> 2. Choose model hyperparameters by instantiating this class with desired values. \n> 3. Arrange data into a features matrix and target vector following the discussion above.\n> 4. Fit the model to your data by calling the `fit()` method of the model instance.\n> 5. Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Linear Algebra!\n\nThe same result that is found by minimizing the sum of the squared errors can be also found through a linear algebra process known as the \"Least Squares Solution:\"\n\n![OLS Regression](http://www.ryanleeallred.com/wp-content/uploads/2018/08/OLS-linear-algebra.png)\n\nBefore we can work with this equation in its linear algebra form we have to understand how to set up the matrices that are involved in this equation. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The $\\beta$ vector\n\nThe $\\beta$ vector represents all the parameters that we are trying to estimate, our $y$ vector and $X$ matrix values are full of data from our dataset. The $\\beta$ vector holds the variables that we are solving for: $\\beta_0$ and $\\beta_1$\n\nNow that we have all of the necessary parts we can set them up in the following equation:\n\n\\begin{align}\ny = X \\beta + \\epsilon\n\\end{align}\n\nSince our $\\epsilon$ value represents **random** error we can assume that it will equal zero on average.\n\n\\begin{align}\ny = X \\beta\n\\end{align}\n\nThe objective now is to isolate the $\\beta$ matrix. We can do this by pre-multiplying both sides by \"X transpose\" $X^{T}$.\n\n\\begin{align}\nX^{T}y =  X^{T}X \\beta\n\\end{align}\n\nSince anything times its transpose will result in a square matrix, if that matrix is then an invertible matrix, then we should be able to multiply both sides by its inverse to remove it from the right hand side. (We'll talk tomorrow about situations that could lead to $X^{T}X$ not being invertible.)\n\n\\begin{align}\n(X^{T}X)^{-1}X^{T}y =  (X^{T}X)^{-1}X^{T}X \\beta\n\\end{align}\n\nSince any matrix multiplied by its inverse results in the identity matrix, and anything multiplied by the identity matrix is itself, we are left with only $\\beta$ on the right hand side:\n\n\\begin{align}\n(X^{T}X)^{-1}X^{T}y = \\hat{\\beta}\n\\end{align}\n\nWe will now call it \"beta hat\" $\\hat{\\beta}$ because it now represents our estimated values for $\\beta_0$ and $\\beta_1$\n\n### Lets calculate our $\\beta$ coefficients with numpy!"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Multiple Regression\n\nSimple or bivariate linear regression involves a single $x$ variable and a single $y$ variable. However, we can have many $x$ variables. A linear regression model that involves multiple x variables is known as **Multiple** Regression (NOT MULTIVARIATE).\n\n![Multiple Regression](http://www.ryanleeallred.com/wp-content/uploads/2018/08/multiple-regression-model.png)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from mpl_toolkits import mplot3d\n\ndef viz3D(fitted_model, df, features, target='', num=100):\n    \"\"\"\n    Visualize model predictions in 3D, for regression or binary classification\n    \n    Parameters\n    ----------\n    fitted_model : scikit-learn model, already fitted\n    df : pandas dataframe, which was used to fit model\n    features : list of strings, name of features 1 & 2\n    target : string, name of target\n    num : int, number of grid points for each feature\n    \n    References\n    ----------\n    https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html\n    https://scikit-learn.org/stable/auto_examples/tree/plot_iris.html  \n    \"\"\"\n    feature1, feature2 = features\n    x1 = np.linspace(df[feature1].min(), df[feature1].max(), num)\n    x2 = np.linspace(df[feature2].min(), df[feature2].max(), num)\n    X1, X2 = np.meshgrid(x1, x2)\n    X = np.c_[X1.flatten(), X2.flatten()]\n    if hasattr(fitted_model, 'predict_proba'):\n        predicted = fitted_model.predict_proba(X)[:,1]\n    else:\n        predicted = fitted_model.predict(X)\n    Z = predicted.reshape(num, num)\n    \n    fig = plt.figure()\n    ax = plt.axes(projection='3d')\n    ax.plot_surface(X1, X2, Z, cmap='viridis')\n    ax.set_xlabel(feature1)\n    ax.set_ylabel(feature2)\n    ax.set_zlabel(target)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Dimensionality in Linear Regression!\n\nMuliple Regression is simply an extension of the bivariate case. The reason why we see the bivariate case demonstrated so often is simply because it's easier to graph and all of the intuition from the bivariate case is the same as we keep on adding explanatory variables.\n\nAs we increase the number of $x$ values in our model we are simply fitting a n-1-dimensional plane to an n-dimensional cloud of points within an n-dimensional hypercube. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Interpreting Coefficients\n\nOne of Linear Regression's strengths is that the parameters of the model (coefficients) are readily interpretable and useful. Not only do they describe the relationship between x and y but they put a number on just how much x is associated with y. We should be careful to not speak about this relationshiop in terms of causality because these coefficients are in fact correlative measures. We would need a host of additional techniques in order to estimate a causal effect using linear regression (econometrics).\n\n\\begin{align}\n\\hat{\\beta} = \\frac{Cov(x,y)}{Var(y)}\n\\end{align}\n\nGoing back to the two equations for the two models that we have estimated so far, lets replace their beta values with their actual values to see if we can make sense of how to interpret these beta coefficients."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Bivariate Model\n\n$y_i = \\beta_0 + \\beta_1temperature + \\epsilon$\n\n$sales_i = -596.2 + 24.69temperature + \\epsilon$\n\nWhat might $\\beta_0$ in this model represent? It represents the level of sales that we would have if temperature were 0. Since this is negative one way of interpreting it is that it's so cold outside that you would have to pay people to eat ice cream. A more appropriate interpretation is probably that the ice cream store owner should close his store down long before the temperature reaches 0 degrees farenheit (-17.7 celsius). The owner can compare his predicted sales with his costs of doing business to know how warm the weather has to get before he should open his store.\n\nWhat might the $beta_1$ in this model reprsent? it represents the increase in sales for each degree of temperature increase. For every degree that the temperature goes up outside he has $25 more in sales."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Multiple Regression Model\n\n$y_i = \\beta_0 + \\beta_1age_i + \\beta_2weight_i + \\epsilon$\n\n$BloodPressure_i = 30.99+ .86age_i + .33weight_i + \\epsilon$\n\nThe interpretation of coefficients in this example are similar. The intercept value repesents the blood pressure a person would have if they were 0 years old and weighed 0 pounds. This not a super useful interpretation. If we look at our data it is unlikely that we have any measurements like these in the dataset. This means that our interpretation of our intercept likely comes from extrapolating the regression line (plane). Coefficients having straightforward interpretations is a strength of linear regression if we're careful about extrapolation and only interpreting our data within the context that it was gathered.\n\nThe interpretation of our other coefficients can be a useful indicator for how much a person similar to those in our dataset's blood pressure will go up on average with each additional year of age and pound of weight."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Basic Model Validation\n\nOne of the downsides of relying on $R^2$ too much is that although it tells you when you're fitting the data well, it doesn't tell you when you're *overfitting* the data. The best way to tell if you're overfitting the data is to get some data that your model hasn't seen yet, and evaluate how your predictions do. This is essentially what \"model validation\" is."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Why is Linear Regression so Important?\n\n## Popularity \n\nLinear Regression is an extremely popular technique that every data scientist **needs** to understand. It's not the most advanced technique and there are supervised learning techniques that will obtain a higher accuracy, but where it lacks in accuracy it makes up for it in interpretability and simplicity.\n\n## Interpretability\n\nFew other models possess coefficients that are so directly linked to their variables with a such a clear interpretation. Tomorrow we're going to learn about ways to make them even easier to interpret.\n\n## Simplicity\n\nA linear regression model can be communicated just by writing out its equation. It's kind of incredible that such high dimensional relationships can be described from just a linear combination of variables and coefficients. "
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}