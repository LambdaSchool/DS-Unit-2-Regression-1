{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4i5n883p_Xf"
   },
   "source": [
    "_Lambda School Data Science — Regression 2_\n",
    "\n",
    "# Doing Linear Regression\n",
    "\n",
    "### Objectives\n",
    "- acquire data for features\n",
    "- arrange data into X features matrix and y target vector\n",
    "- begin with baselines for regression\n",
    "- use scikit-learn for linear regression\n",
    "- use regression metric: MAE\n",
    "- do leave-one-out cross-validation\n",
    "\n",
    "### Contents\n",
    "1. Pre-reads\n",
    "2. Process\n",
    "3. Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6O0KEmmQ7OM"
   },
   "source": [
    "# Pre-reads\n",
    "\n",
    "#### [Jake VanderPlas, Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html). \n",
    "\n",
    "Read up through “Supervised learning example: Simple linear regression”. You can stop when you get to “Supervised learning example: Iris classification.”\n",
    "\n",
    "#### [Nate Silver, What Do Economic Models Really Tell Us About Elections?](https://fivethirtyeight.com/features/what-do-economic-models-really-tell-us-about-elections/)\n",
    "\n",
    "Read the whole thing. We’ll make a model similar to the “Bread and Peace” model...\n",
    "\n",
    ">Perhaps the best-known of these models is the so-called “Bread and Peace” model designed by Douglas Hibbs of the University of Gothenberg. There are a lot of things to admire about this model. Most notably, it’s not larded down with superfluous variables. Instead, it is based on just two: growth in real, per-capita disposable income, and the number of military fatalities resulting from U.S.-initiated foreign conflicts.\n",
    "\n",
    "... and then you’ll make your own elections model, with two features of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZ7Oh030150T"
   },
   "source": [
    "# Process\n",
    "\n",
    "#### Renee Teate, [Becoming a Data Scientist, PyData DC 2016 Talk](https://www.becomingadatascientist.com/2016/10/11/pydata-dc-2016-talk/)\n",
    "\n",
    "![](https://image.slidesharecdn.com/becomingadatascientistadvice-pydatadc-shared-161012184823/95/becoming-a-data-scientist-advice-from-my-podcast-guests-55-638.jpg?cb=1476298295)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEZu7RSd0O3w"
   },
   "source": [
    "## Business Question --> Data Question --> Data Answer (for Supervised Learning)\n",
    "\n",
    "#### Francois Chollet, [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/README.md), Chapter 4: Fundamentals of machine learning, \"A universal workflow of machine learning\"\n",
    " \n",
    "> **1. Define the problem at hand and the data on which you’ll train.** Collect this data, or annotate it with labels if need be.\n",
    "\n",
    "> **2. Choose how you’ll measure success on your problem.** Which metrics will you monitor on your validation data?\n",
    "\n",
    "> **3. Determine your evaluation protocol:** hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
    "\n",
    "> **4. Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
    "\n",
    "> **5. Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
    "\n",
    "> **6. Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. \n",
    "\n",
    "> **Iterate on feature engineering: add new features, or remove features that don’t seem to be informative.** Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJhnVFGQRXS0"
   },
   "source": [
    "## Define the data on which you'll train / Add new features or remove features\n",
    "\n",
    "#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Data Representation in Scikit-Learn\n",
    "\n",
    "> The best way to think about data within Scikit-Learn is in terms of tables of data.\n",
    "\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.02-samples-features.png)\n",
    "\n",
    "> The samples (i.e., rows) always refer to the individual objects described by the dataset. For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.\n",
    "\n",
    "> The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner. \n",
    "\n",
    "> The information can be thought of as a two-dimensional numerical array or matrix, which we will call the _features matrix._ By convention, this features matrix is often stored in a variable named `X`. The features matrix is assumed to be two-dimensional, with shape `[n_samples, n_features]`, and is most often contained in a NumPy array or a Pandas `DataFrame`, though some Scikit-Learn models also accept SciPy sparse matrices.\n",
    "\n",
    "> In addition to the feature matrix `X`, we also generally work with a label or target array, which by convention we will usually call `y`. The target array is usually one dimensional, with length `n_samples`, and is generally contained in a NumPy array or Pandas `Series`. \n",
    "\n",
    "> Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to _predict from the data:_ in statistical terms, it is the dependent variable.\n",
    "\n",
    "#### Google Developers, [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/#l) \n",
    "\n",
    "> Each example in a labeled dataset consists of one or more features and a label. \n",
    "\n",
    "> For instance, in a housing dataset, the features might include the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the house's price. \n",
    "\n",
    "> In a spam detection dataset, the features might include the subject line, the sender, and the email message itself, while the label would probably be either \"spam\" or \"not spam.\"\n",
    "\n",
    "#### Wikipedia, [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSZBdwNg1Vvj"
   },
   "source": [
    "## Determine evaluation protocol\n",
    "\n",
    "#### Sebastian Raschka, [Model Evaluation]( https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)\n",
    "> <img src=\"https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFAHa5dD3BQt"
   },
   "source": [
    "## Develop a first model that does better than a basic baseline\n",
    "\n",
    "### Why begin with baselines?\n",
    "\n",
    "[My mentor](https://www.linkedin.com/in/jason-sanchez-62093847/) [taught me](https://youtu.be/0GrciaGYzV0?t=40s):\n",
    "\n",
    ">***Your first goal should always, always, always be getting a generalized prediction as fast as possible.*** You shouldn't spend a lot of time trying to tune your model, trying to add features, trying to engineer features, until you've actually gotten one prediction, at least. \n",
    "\n",
    "> The reason why that's a really good thing is because then ***you'll set a benchmark*** for yourself, and you'll be able to directly see how much effort you put in translates to a better prediction. \n",
    "\n",
    "> What you'll find by working on many models: some effort you put in, actually has very little effect on how well your final model does at predicting new observations. Whereas some very easy changes actually have a lot of effect. And so you get better at allocating your time more effectively.\n",
    "\n",
    "My mentor's advice is echoed and elaborated in several sources:\n",
    "\n",
    "[Always start with a stupid model, no exceptions](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa)\n",
    "\n",
    "> Why start with a baseline? A baseline will take you less than 1/10th of the time, and could provide up to 90% of the results. A baseline puts a more complex model into context. Baselines are easy to deploy.\n",
    "\n",
    "[Measure Once, Cut Twice: Moving Towards Iteration in Data Science](https://blog.datarobot.com/measure-once-cut-twice-moving-towards-iteration-in-data-science)\n",
    "\n",
    "> The iterative approach in data science starts with emphasizing the importance of getting to a first model quickly, rather than starting with the variables and features. Once the first model is built, the work then steadily focuses on continual improvement.\n",
    "\n",
    "[*Data Science for Business*](https://books.google.com/books?id=4ZctAAAAQBAJ&pg=PT276), Chapter 7.3: Evaluation, Baseline Performance, and Implications for Investments in Data\n",
    "\n",
    "> *Consider carefully what would be a reasonable baseline against which to compare model performance.* This is important for the data science team in order to understand whether they indeed are improving performance, and is equally important for demonstrating to stakeholders that mining the data has added value.\n",
    "\n",
    "### What does baseline mean?\n",
    "\n",
    "Baseline is an overloaded term, as you can see in the links above. Baseline has multiple meanings:\n",
    "\n",
    "#### The score you'd get by guessing a single value\n",
    "\n",
    "> A baseline for classification can be the most common class in the training dataset.\n",
    "\n",
    "> A baseline for regression can be the mean of the training labels. —[Will Koehrsen](https://twitter.com/koehrsen_will/status/1088863527778111488)\n",
    "\n",
    "#### The score you'd get by guessing in a more granular way\n",
    "\n",
    "> A baseline for time-series regressions can be the value from the previous timestep.\n",
    "\n",
    "#### Fast, first models that beat guessing\n",
    "\n",
    "What my mentor was talking about.\n",
    "\n",
    "#### Complete, tuned \"simpler\" model\n",
    "\n",
    "Can be simpler mathematically and computationally. For example, Logistic Regression versus Deep Learning.\n",
    "\n",
    "Or can be simpler for the data scientist, with less work. For example, a model with less feature engineering versus a model with more feature engineering.\n",
    "\n",
    "#### Minimum performance that \"matters\"\n",
    "\n",
    "To go to production and get business value.\n",
    "\n",
    "#### Human-level performance \n",
    "\n",
    "Your goal may to be match, or nearly match, human performance, but with better speed, cost, or consistency.\n",
    "\n",
    "Or your goal may to be exceed human performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOW2hsiDBIk9"
   },
   "source": [
    "## Use scikit-learn to fit a model\n",
    "\n",
    "#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "> Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n",
    "\n",
    "> 1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn. \n",
    "> 2. Choose model hyperparameters by instantiating this class with desired values. \n",
    "> 3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "> 4. Fit the model to your data by calling the `fit()` method of the model instance.\n",
    "> 5. Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1hPspa5mkWT"
   },
   "source": [
    "# Project\n",
    "\n",
    "### Predict presidential election voting, doing linear regression with two features\n",
    "\n",
    "#### Douglas Hibbs, [Background Information on the ‘Bread and Peace’ Model of Voting in Postwar US Presidential Elections](https://douglas-hibbs.com/background-information-on-bread-and-peace-voting-in-us-presidential-elections/)\n",
    "\n",
    "> According to the ‘Bread and Peace’ model, postwar US presidential elections can for the most part be interpreted as a sequence of referendums on the incumbent party’s record during its four-year mandate period. \n",
    "\n",
    "> In fact aggregate two-party vote shares going to candidates of the party holding the presidency during the postwar era are well explained by just two fundamental determinants:\n",
    "\n",
    "> (1) Positively by weighted-average growth of per capita real disposable personal income over the term.\n",
    "\n",
    "> (2) Negatively by cumulative US military fatalities (scaled to population) owing to unprovoked, hostile deployments of American armed forces in foreign wars.\n",
    "\n",
    "![](https://douglas-hibbs.com/wp-content/uploads/2013/08/BP1v4c2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbqeAzl9TK_y"
   },
   "source": [
    "## Define the data on which you'll train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5Cl5iTHXwA7"
   },
   "source": [
    "### Load data\n",
    "\n",
    "#### Sources\n",
    "- 1952-2012: Douglas Hibbs, [2014 lecture at Deakin University Melbourne](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 40\n",
    "- 2016, Vote Share: [The American Presidency Project](https://www.presidency.ucsb.edu/statistics/elections)\n",
    "- 2016, Recent Growth in Personal Incomes: [The 2016 election economy: the \"Bread and Peace\" model final forecast](https://angrybearblog.com/2016/11/the-2016-election-economy-the-bread-and-peace-model-final-forecast.html)\n",
    "- 2016, US Military Fatalities: Assumption that Afghanistan War fatalities in 2012-16 occured at the same rate as 2008-12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEeVFTa0VWDE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['Year','Incumbent Party Candidate','Other Candidate','Incumbent Party Vote Share']\n",
    "\n",
    "data = [[1952,\"Stevenson\",\"Eisenhower\",44.6],\n",
    "        [1956,\"Eisenhower\",\"Stevenson\",57.76],\n",
    "        [1960,\"Nixon\",\"Kennedy\",49.91],\n",
    "        [1964,\"Johnson\",\"Goldwater\",61.34],\n",
    "        [1968,\"Humphrey\",\"Nixon\",49.60],\n",
    "        [1972,\"Nixon\",\"McGovern\",61.79],\n",
    "        [1976,\"Ford\",\"Carter\",48.95],\n",
    "        [1980,\"Carter\",\"Reagan\",44.70],\n",
    "        [1984,\"Reagan\",\"Mondale\",59.17],\n",
    "        [1988,\"Bush, Sr.\",\"Dukakis\",53.94],\n",
    "        [1992,\"Bush, Sr.\",\"Clinton\",46.55],\n",
    "        [1996,\"Clinton\",\"Dole\",54.74],\n",
    "        [2000,\"Gore\",\"Bush, Jr.\",50.27],\n",
    "        [2004,\"Bush, Jr.\",\"Kerry\",51.24],\n",
    "        [2008,\"McCain\",\"Obama\",46.32],\n",
    "        [2012,\"Obama\",\"Romney\",52.00], \n",
    "        [2016,\"Clinton\",\"Trump\",48.2]]\n",
    "        \n",
    "votes = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNKTk8bV7c7U"
   },
   "outputs": [],
   "source": [
    "columns = ['Year','Average Recent Growth in Personal Incomes']\n",
    "\n",
    "data = [[1952,2.40],\n",
    "        [1956,2.89],\n",
    "        [1960, .85],\n",
    "        [1964,4.21],\n",
    "        [1968,3.02],\n",
    "        [1972,3.62],\n",
    "        [1976,1.08],\n",
    "        [1980,-.39],\n",
    "        [1984,3.86],\n",
    "        [1988,2.27],\n",
    "        [1992, .38],\n",
    "        [1996,1.04],\n",
    "        [2000,2.36],\n",
    "        [2004,1.72],\n",
    "        [2008, .10],\n",
    "        [2012, .95], \n",
    "        [2016, .10]]\n",
    "        \n",
    "growth = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxyoxOLQl7VX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fatalities denotes the cumulative number of American military\n",
    "fatalities per millions of US population the in Korea, Vietnam,\n",
    "Iraq and Afghanistan wars during the presidential terms\n",
    "preceding the 1952, 1964, 1968, 1976 and 2004, 2008 and\n",
    "2012 elections.\n",
    "\n",
    "http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf\n",
    "\"\"\"\n",
    "\n",
    "columns = ['Year','US Military Fatalities per Million']\n",
    "\n",
    "data = [[1952,190],\n",
    "        [1956,  0],\n",
    "        [1960,  0],\n",
    "        [1964,  1],\n",
    "        [1968,146],\n",
    "        [1972,  0],\n",
    "        [1976,  2],\n",
    "        [1980,  0],\n",
    "        [1984,  0],\n",
    "        [1988,  0],\n",
    "        [1992,  0],\n",
    "        [1996,  0],\n",
    "        [2000,  0],\n",
    "        [2004,  4],\n",
    "        [2008, 14],\n",
    "        [2012,  5], \n",
    "        [2016,  5]]\n",
    "        \n",
    "deaths = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXirDjeR73Bm"
   },
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TWFDN_QZ_DM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17, 4), (17, 2), (17, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes.shape, growth.shape, deaths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Year', 'Incumbent Party Candidate', 'Other Candidate',\n",
       "        'Incumbent Party Vote Share'],\n",
       "       dtype='object'),\n",
       " Index(['Year', 'Average Recent Growth in Personal Incomes'], dtype='object'),\n",
       " Index(['Year', 'US Military Fatalities per Million'], dtype='object'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes.columns, growth.columns, deaths.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = votes.merge(growth).merge(deaths)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Incumbent Party Candidate</th>\n",
       "      <th>Other Candidate</th>\n",
       "      <th>Incumbent Party Vote Share</th>\n",
       "      <th>Average Recent Growth in Personal Incomes</th>\n",
       "      <th>US Military Fatalities per Million</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1952</td>\n",
       "      <td>Stevenson</td>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>44.60</td>\n",
       "      <td>2.40</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956</td>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>Stevenson</td>\n",
       "      <td>57.76</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>49.91</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1964</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>Goldwater</td>\n",
       "      <td>61.34</td>\n",
       "      <td>4.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1968</td>\n",
       "      <td>Humphrey</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>49.60</td>\n",
       "      <td>3.02</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Incumbent Party Candidate Other Candidate  Incumbent Party Vote Share  \\\n",
       "0  1952                 Stevenson      Eisenhower                       44.60   \n",
       "1  1956                Eisenhower       Stevenson                       57.76   \n",
       "2  1960                     Nixon         Kennedy                       49.91   \n",
       "3  1964                   Johnson       Goldwater                       61.34   \n",
       "4  1968                  Humphrey           Nixon                       49.60   \n",
       "\n",
       "   Average Recent Growth in Personal Incomes  \\\n",
       "0                                       2.40   \n",
       "1                                       2.89   \n",
       "2                                       0.85   \n",
       "3                                       4.21   \n",
       "4                                       3.02   \n",
       "\n",
       "   US Military Fatalities per Million  \n",
       "0                                 190  \n",
       "1                                   0  \n",
       "2                                   0  \n",
       "3                                   1  \n",
       "4                                 146  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMmPjCEWTXFn"
   },
   "source": [
    "## Begin with baselines for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9aec8nw-Ybh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17.000000\n",
       "mean     51.828235\n",
       "std       5.510739\n",
       "min      44.600000\n",
       "25%      48.200000\n",
       "50%      50.270000\n",
       "75%      54.740000\n",
       "max      61.790000\n",
       "Name: Incumbent Party Vote Share, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Incumbent Party Vote Share'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.distplot(df['Incumbent Party Vote Share']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Incumbent Party Candidate</th>\n",
       "      <th>Other Candidate</th>\n",
       "      <th>Incumbent Party Vote Share</th>\n",
       "      <th>Average Recent Growth in Personal Incomes</th>\n",
       "      <th>US Military Fatalities per Million</th>\n",
       "      <th>Mean Baseline</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1952</td>\n",
       "      <td>Stevenson</td>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>44.60</td>\n",
       "      <td>2.40</td>\n",
       "      <td>190</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>7.228235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956</td>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>Stevenson</td>\n",
       "      <td>57.76</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-5.931765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>49.91</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>1.918235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1964</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>Goldwater</td>\n",
       "      <td>61.34</td>\n",
       "      <td>4.21</td>\n",
       "      <td>1</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-9.511765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1968</td>\n",
       "      <td>Humphrey</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>49.60</td>\n",
       "      <td>3.02</td>\n",
       "      <td>146</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>2.228235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1972</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>McGovern</td>\n",
       "      <td>61.79</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-9.961765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1976</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Carter</td>\n",
       "      <td>48.95</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>2.878235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1980</td>\n",
       "      <td>Carter</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>44.70</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>7.128235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1984</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>Mondale</td>\n",
       "      <td>59.17</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-7.341765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1988</td>\n",
       "      <td>Bush, Sr.</td>\n",
       "      <td>Dukakis</td>\n",
       "      <td>53.94</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-2.111765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1992</td>\n",
       "      <td>Bush, Sr.</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>46.55</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>5.278235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1996</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Dole</td>\n",
       "      <td>54.74</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-2.911765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000</td>\n",
       "      <td>Gore</td>\n",
       "      <td>Bush, Jr.</td>\n",
       "      <td>50.27</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>1.558235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2004</td>\n",
       "      <td>Bush, Jr.</td>\n",
       "      <td>Kerry</td>\n",
       "      <td>51.24</td>\n",
       "      <td>1.72</td>\n",
       "      <td>4</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008</td>\n",
       "      <td>McCain</td>\n",
       "      <td>Obama</td>\n",
       "      <td>46.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>14</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>5.508235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Romney</td>\n",
       "      <td>52.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-0.171765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Trump</td>\n",
       "      <td>48.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>3.628235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year Incumbent Party Candidate Other Candidate  \\\n",
       "0   1952                 Stevenson      Eisenhower   \n",
       "1   1956                Eisenhower       Stevenson   \n",
       "2   1960                     Nixon         Kennedy   \n",
       "3   1964                   Johnson       Goldwater   \n",
       "4   1968                  Humphrey           Nixon   \n",
       "5   1972                     Nixon        McGovern   \n",
       "6   1976                      Ford          Carter   \n",
       "7   1980                    Carter          Reagan   \n",
       "8   1984                    Reagan         Mondale   \n",
       "9   1988                 Bush, Sr.         Dukakis   \n",
       "10  1992                 Bush, Sr.         Clinton   \n",
       "11  1996                   Clinton            Dole   \n",
       "12  2000                      Gore       Bush, Jr.   \n",
       "13  2004                 Bush, Jr.           Kerry   \n",
       "14  2008                    McCain           Obama   \n",
       "15  2012                     Obama          Romney   \n",
       "16  2016                   Clinton           Trump   \n",
       "\n",
       "    Incumbent Party Vote Share  Average Recent Growth in Personal Incomes  \\\n",
       "0                        44.60                                       2.40   \n",
       "1                        57.76                                       2.89   \n",
       "2                        49.91                                       0.85   \n",
       "3                        61.34                                       4.21   \n",
       "4                        49.60                                       3.02   \n",
       "5                        61.79                                       3.62   \n",
       "6                        48.95                                       1.08   \n",
       "7                        44.70                                      -0.39   \n",
       "8                        59.17                                       3.86   \n",
       "9                        53.94                                       2.27   \n",
       "10                       46.55                                       0.38   \n",
       "11                       54.74                                       1.04   \n",
       "12                       50.27                                       2.36   \n",
       "13                       51.24                                       1.72   \n",
       "14                       46.32                                       0.10   \n",
       "15                       52.00                                       0.95   \n",
       "16                       48.20                                       0.10   \n",
       "\n",
       "    US Military Fatalities per Million  Mean Baseline     Error  \n",
       "0                                  190      51.828235  7.228235  \n",
       "1                                    0      51.828235 -5.931765  \n",
       "2                                    0      51.828235  1.918235  \n",
       "3                                    1      51.828235 -9.511765  \n",
       "4                                  146      51.828235  2.228235  \n",
       "5                                    0      51.828235 -9.961765  \n",
       "6                                    2      51.828235  2.878235  \n",
       "7                                    0      51.828235  7.128235  \n",
       "8                                    0      51.828235 -7.341765  \n",
       "9                                    0      51.828235 -2.111765  \n",
       "10                                   0      51.828235  5.278235  \n",
       "11                                   0      51.828235 -2.911765  \n",
       "12                                   0      51.828235  1.558235  \n",
       "13                                   4      51.828235  0.588235  \n",
       "14                                  14      51.828235  5.508235  \n",
       "15                                   5      51.828235 -0.171765  \n",
       "16                                   5      51.828235  3.628235  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'Incumbent Party Vote Share'\n",
    "df['Mean Baseline'] = df[target].mean()\n",
    "df['Error'] = df['Mean Baseline'] - df[target]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4210854715202004e-14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a wrong way \n",
    "df['Error'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.88470588235293"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aright way\n",
    "df['Absolute Error'] = df['Error'].abs()\n",
    "df['Absolute Error'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3hDX7yUTbix"
   },
   "source": [
    "## Use scikit-learn for linear regression, with 1 feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFWAop61CgCq"
   },
   "source": [
    "Follow the process from Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "### Choose a class of model by importing the appropriate estimator class from Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMbOVWEDCfmO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vATSdu5oD5NQ"
   },
   "source": [
    "### Choose model hyperparameters by instantiating this class with desired values\n",
    "\n",
    "Refer to scikit-learn documentation to see what model hyperparameters you can choose. For example: [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CexmSzauEBnu"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEsa7jtHC0L5"
   },
   "source": [
    "### Arrange data into X features matrix and y target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Average Recent Growth in Personal Incomes']\n",
    "target = 'Incumbent Party Vote Share'\n",
    "X = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8s3-WYWEKxN"
   },
   "source": [
    "### Fit the model to your data by calling the `fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTLnEzwUENb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.97417709]), 46.499209757741625)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HynZZRL7ESvx"
   },
   "source": [
    "### Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCtwohVKERED"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6l-WbivHIwHh"
   },
   "source": [
    "## Use regression metric: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfuI9wu5FvXu"
   },
   "outputs": [],
   "source": [
    "df['Linear Regression, 1 feature'] = y_pred\n",
    "df['Error'] = df['Linear Regression, 1 feature'] - df[target]\n",
    "df['Absolute Error'] = df['Error'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Incumbent Party Candidate</th>\n",
       "      <th>Other Candidate</th>\n",
       "      <th>Incumbent Party Vote Share</th>\n",
       "      <th>Average Recent Growth in Personal Incomes</th>\n",
       "      <th>US Military Fatalities per Million</th>\n",
       "      <th>Mean Baseline</th>\n",
       "      <th>Error</th>\n",
       "      <th>Absolute Error</th>\n",
       "      <th>Linear Regression, 1 feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1952</td>\n",
       "      <td>Stevenson</td>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>44.60</td>\n",
       "      <td>2.40</td>\n",
       "      <td>190</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>9.037235</td>\n",
       "      <td>9.037235</td>\n",
       "      <td>53.637235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956</td>\n",
       "      <td>Eisenhower</td>\n",
       "      <td>Stevenson</td>\n",
       "      <td>57.76</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-2.665418</td>\n",
       "      <td>2.665418</td>\n",
       "      <td>55.094582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>49.91</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-0.882740</td>\n",
       "      <td>0.882740</td>\n",
       "      <td>49.027260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1964</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>Goldwater</td>\n",
       "      <td>61.34</td>\n",
       "      <td>4.21</td>\n",
       "      <td>1</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-2.319505</td>\n",
       "      <td>2.319505</td>\n",
       "      <td>59.020495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1968</td>\n",
       "      <td>Humphrey</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>49.60</td>\n",
       "      <td>3.02</td>\n",
       "      <td>146</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>5.881225</td>\n",
       "      <td>5.881225</td>\n",
       "      <td>55.481225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1972</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>McGovern</td>\n",
       "      <td>61.79</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-4.524269</td>\n",
       "      <td>4.524269</td>\n",
       "      <td>57.265731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1976</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Carter</td>\n",
       "      <td>48.95</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>0.761321</td>\n",
       "      <td>0.761321</td>\n",
       "      <td>49.711321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1980</td>\n",
       "      <td>Carter</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>44.70</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>0.639281</td>\n",
       "      <td>0.639281</td>\n",
       "      <td>45.339281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1984</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>Mondale</td>\n",
       "      <td>59.17</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-1.190467</td>\n",
       "      <td>1.190467</td>\n",
       "      <td>57.979533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1988</td>\n",
       "      <td>Bush, Sr.</td>\n",
       "      <td>Dukakis</td>\n",
       "      <td>53.94</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-0.689408</td>\n",
       "      <td>0.689408</td>\n",
       "      <td>53.250592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1992</td>\n",
       "      <td>Bush, Sr.</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>46.55</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>1.079397</td>\n",
       "      <td>1.079397</td>\n",
       "      <td>47.629397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1996</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Dole</td>\n",
       "      <td>54.74</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-5.147646</td>\n",
       "      <td>5.147646</td>\n",
       "      <td>49.592354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000</td>\n",
       "      <td>Gore</td>\n",
       "      <td>Bush, Jr.</td>\n",
       "      <td>50.27</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>3.248268</td>\n",
       "      <td>3.248268</td>\n",
       "      <td>53.518268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2004</td>\n",
       "      <td>Bush, Jr.</td>\n",
       "      <td>Kerry</td>\n",
       "      <td>51.24</td>\n",
       "      <td>1.72</td>\n",
       "      <td>4</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>0.374794</td>\n",
       "      <td>0.374794</td>\n",
       "      <td>51.614794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008</td>\n",
       "      <td>McCain</td>\n",
       "      <td>Obama</td>\n",
       "      <td>46.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>14</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>0.476627</td>\n",
       "      <td>0.476627</td>\n",
       "      <td>46.796627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012</td>\n",
       "      <td>Obama</td>\n",
       "      <td>Romney</td>\n",
       "      <td>52.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-2.675322</td>\n",
       "      <td>2.675322</td>\n",
       "      <td>49.324678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Trump</td>\n",
       "      <td>48.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>51.828235</td>\n",
       "      <td>-1.403373</td>\n",
       "      <td>1.403373</td>\n",
       "      <td>46.796627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year Incumbent Party Candidate Other Candidate  \\\n",
       "0   1952                 Stevenson      Eisenhower   \n",
       "1   1956                Eisenhower       Stevenson   \n",
       "2   1960                     Nixon         Kennedy   \n",
       "3   1964                   Johnson       Goldwater   \n",
       "4   1968                  Humphrey           Nixon   \n",
       "5   1972                     Nixon        McGovern   \n",
       "6   1976                      Ford          Carter   \n",
       "7   1980                    Carter          Reagan   \n",
       "8   1984                    Reagan         Mondale   \n",
       "9   1988                 Bush, Sr.         Dukakis   \n",
       "10  1992                 Bush, Sr.         Clinton   \n",
       "11  1996                   Clinton            Dole   \n",
       "12  2000                      Gore       Bush, Jr.   \n",
       "13  2004                 Bush, Jr.           Kerry   \n",
       "14  2008                    McCain           Obama   \n",
       "15  2012                     Obama          Romney   \n",
       "16  2016                   Clinton           Trump   \n",
       "\n",
       "    Incumbent Party Vote Share  Average Recent Growth in Personal Incomes  \\\n",
       "0                        44.60                                       2.40   \n",
       "1                        57.76                                       2.89   \n",
       "2                        49.91                                       0.85   \n",
       "3                        61.34                                       4.21   \n",
       "4                        49.60                                       3.02   \n",
       "5                        61.79                                       3.62   \n",
       "6                        48.95                                       1.08   \n",
       "7                        44.70                                      -0.39   \n",
       "8                        59.17                                       3.86   \n",
       "9                        53.94                                       2.27   \n",
       "10                       46.55                                       0.38   \n",
       "11                       54.74                                       1.04   \n",
       "12                       50.27                                       2.36   \n",
       "13                       51.24                                       1.72   \n",
       "14                       46.32                                       0.10   \n",
       "15                       52.00                                       0.95   \n",
       "16                       48.20                                       0.10   \n",
       "\n",
       "    US Military Fatalities per Million  Mean Baseline     Error  \\\n",
       "0                                  190      51.828235  9.037235   \n",
       "1                                    0      51.828235 -2.665418   \n",
       "2                                    0      51.828235 -0.882740   \n",
       "3                                    1      51.828235 -2.319505   \n",
       "4                                  146      51.828235  5.881225   \n",
       "5                                    0      51.828235 -4.524269   \n",
       "6                                    2      51.828235  0.761321   \n",
       "7                                    0      51.828235  0.639281   \n",
       "8                                    0      51.828235 -1.190467   \n",
       "9                                    0      51.828235 -0.689408   \n",
       "10                                   0      51.828235  1.079397   \n",
       "11                                   0      51.828235 -5.147646   \n",
       "12                                   0      51.828235  3.248268   \n",
       "13                                   4      51.828235  0.374794   \n",
       "14                                  14      51.828235  0.476627   \n",
       "15                                   5      51.828235 -2.675322   \n",
       "16                                   5      51.828235 -1.403373   \n",
       "\n",
       "    Absolute Error  Linear Regression, 1 feature  \n",
       "0         9.037235                     53.637235  \n",
       "1         2.665418                     55.094582  \n",
       "2         0.882740                     49.027260  \n",
       "3         2.319505                     59.020495  \n",
       "4         5.881225                     55.481225  \n",
       "5         4.524269                     57.265731  \n",
       "6         0.761321                     49.711321  \n",
       "7         0.639281                     45.339281  \n",
       "8         1.190467                     57.979533  \n",
       "9         0.689408                     53.250592  \n",
       "10        1.079397                     47.629397  \n",
       "11        5.147646                     49.592354  \n",
       "12        3.248268                     53.518268  \n",
       "13        0.374794                     51.614794  \n",
       "14        0.476627                     46.796627  \n",
       "15        2.675322                     49.324678  \n",
       "16        1.403373                     46.796627  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_KGM3LOHyrW"
   },
   "source": [
    "## Use scikit-learn for linear regression, with 2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_79qOeAH2ZU"
   },
   "source": [
    "Follow the process from Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "### Choose a class of model by importing the appropriate estimator class from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E26qduGiH2_y"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--7julXYH3oC"
   },
   "source": [
    "### Choose model hyperparameters by instantiating this class with desired values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkkoMxbsIXLR"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJNRFvK9IeWU"
   },
   "source": [
    "### Arrange data into X features matrix and y target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2290BJszIgrb"
   },
   "outputs": [],
   "source": [
    "features = ['Average Recent Growth in Personal Incomes', 'US Military Fatalities per Million']\n",
    "\n",
    "target = 'Incumbent Party Vote Share'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOa5Uj4jIjDR"
   },
   "source": [
    "### Fit the model to your data by calling the `fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZAUSsY0IjWa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GEyW2B3Imr2"
   },
   "source": [
    "### Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ubKZVRJInLV"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDbG8jreI8Ip"
   },
   "source": [
    "## Use regression metric: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCs--47RI-He"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3975663494016113"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Linear Regression, 2 features'] = y_pred\n",
    "df['Error'] = df['Linear Regression, 2 features'] - df[target]\n",
    "\n",
    "\n",
    "df['Linear Regression, 1 feature'] = y_pred\n",
    "df['Error'] = df['Linear Regression, 1 feature'] - df[target]\n",
    "df['Absolute Error'] = df['Error'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZp_TyR9Tg9A"
   },
   "source": [
    "## Do leave-one-out cross-validation\n",
    "\n",
    "[Nate Silver's post on economic elections models](https://fivethirtyeight.com/features/what-do-economic-models-really-tell-us-about-elections/) discusses out-of-sample testing.\n",
    "\n",
    "[Sebastian Raschka's chart](https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg) shows that \"leave-one-out cross-validation\" is an option for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avlXCUIAI-lL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Linear Regression on all years except 1952\n",
      "Absolute Error on prediction for 1952 = 0.6710286794539044\n",
      "Fit Linear Regression on all years except 1956\n",
      "Absolute Error on prediction for 1956 = 1.1652530150273535\n",
      "Fit Linear Regression on all years except 1960\n",
      "Absolute Error on prediction for 1960 = 0.14204849628617922\n",
      "Fit Linear Regression on all years except 1964\n",
      "Absolute Error on prediction for 1964 = 0.23142167116768775\n",
      "Fit Linear Regression on all years except 1968\n",
      "Absolute Error on prediction for 1968 = 0.44497540045065165\n",
      "Fit Linear Regression on all years except 1972\n",
      "Absolute Error on prediction for 1972 = 3.1780175021763384\n",
      "Fit Linear Regression on all years except 1976\n",
      "Absolute Error on prediction for 1976 = 1.635190786562319\n",
      "Fit Linear Regression on all years except 1980\n",
      "Absolute Error on prediction for 1980 = 1.0804837297850156\n",
      "Fit Linear Regression on all years except 1984\n",
      "Absolute Error on prediction for 1984 = 1.1100047214006281\n",
      "Fit Linear Regression on all years except 1988\n",
      "Absolute Error on prediction for 1988 = 0.73613108287514\n",
      "Fit Linear Regression on all years except 1992\n",
      "Absolute Error on prediction for 1992 = 1.858082182826223\n",
      "Fit Linear Regression on all years except 1996\n",
      "Absolute Error on prediction for 1996 = 4.694196191616342\n",
      "Fit Linear Regression on all years except 2000\n",
      "Absolute Error on prediction for 2000 = 5.076459554820929\n",
      "Fit Linear Regression on all years except 2004\n",
      "Absolute Error on prediction for 2004 = 1.3788977163226477\n",
      "Fit Linear Regression on all years except 2008\n",
      "Absolute Error on prediction for 2008 = 0.180357093017129\n",
      "Fit Linear Regression on all years except 2012\n",
      "Absolute Error on prediction for 2012 = 2.3412315153300085\n",
      "Fit Linear Regression on all years except 2016\n",
      "Absolute Error on prediction for 2016 = 1.458044197718813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "for year in df['Year']: \n",
    "    print(f'Fit Linear Regression on all years except {year}')\n",
    "    \n",
    "    # Choose model hyperparameters by instantiating this class\n",
    "    model = LinearRegression()\n",
    "    # Arrange data into X features matrix and y target vector\n",
    "    train = df[df['Year'] != year]\n",
    "    test = df[df['Year'] == year]\n",
    "    X_train = train[features]\n",
    "    y_train = train[target]\n",
    "    X_test = test[features]\n",
    "    y_test = test[target]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f'Absolute Error on prediction for {year} = {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxbxANM-JHdv"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "#### Predict presidential election voting, with two features you choose!\n",
    "- Start a new notebook.\n",
    "- You may reuse one of the features from the \"Bread & Peace\" model.\n",
    "- **Acquire data for at least one new feature.** The links below may help!\n",
    "- Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "#### Why I'm asking you to acquire data for at least one new [feature](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. \n",
    "\n",
    "#### You can search [FRED (Federal Reserve Bank of St. Louis)](https://fred.stlouisfed.org/) for these keywords\n",
    "- real disposable income change annual\n",
    "- gdp change annual\n",
    "- unemployment \n",
    "\n",
    "#### Go to [BEA (Bureau of Economic Analysis)](https://apps.bea.gov/itable/) and follow these steps\n",
    "- National Data - GDP & Personal Income\n",
    "- Begin using the data\n",
    "- Section 1 - DOMESTIC PRODUCT AND INCOME\n",
    "- Table 1.17.1. Percent Change From Preceding Period in Real Gross Domestic Product, Real Gross Domestic Income, and Other Major NIPA Aggregates\n",
    "- Modify\n",
    "  - First Year: 1947\n",
    "  - Last Year: 2018\n",
    "  - Series: Annual\n",
    "  - Refresh Table\n",
    "- Download\n",
    "\n",
    "#### Go to Wikipedia, [United States military casualties of war, Wars ranked by total number of U.S. military deaths](https://en.wikipedia.org/wiki/United_States_military_casualties_of_war#Wars_ranked_by_total_number_of_U.S._military_deaths)\n",
    "- You can try this tutorial to scrape data from HTML tables: [Quick Tip: The easiest way to grab data out of a web page in Python](https://medium.com/@ageitgey/quick-tip-the-easiest-way-to-grab-data-out-of-a-web-page-in-python-7153cecfca58)\n",
    "\n",
    "#### Read more about economic features to predict elections\n",
    "- [Which Economic Indicators Best Predict Presidential Elections?](https://fivethirtyeight.blogs.nytimes.com/2011/11/18/which-economic-indicators-best-predict-presidential-elections/)\n",
    "- [What stat best gets at the question, \"Are you better off now than you were a year ago?\"](https://www.theatlantic.com/business/archive/2010/11/the-most-important-economic-indicator-in-midterm-elections/65505/)\n",
    "- [Time for change model](https://pollyvote.com/en/components/econometric-models/time-for-change-model/)\n",
    "\n",
    "\n",
    "#### You can try for a \"serious\" model or a \"spurious\" model. Here are more data sources you can try\n",
    "- [Tyler Vigen, Spurious Correlations, Discover a Correlation](https://tylervigen.com/discover)\n",
    "- [CDC (Centers for Disease Control), Compressed Mortality data](https://wonder.cdc.gov/mortSQL.html)\n",
    "- [Data Is Plural](https://tinyletter.com/data-is-plural)\n",
    "- [Gapminder](https://github.com/open-numbers/ddf--gapminder--systema_globalis/)\n",
    "- [Campaign Finance Institute, Historical Stats](http://www.cfinst.org/data/historicalstats.aspx)\n",
    "- Or find your own data and features to try!\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "doing_linear_regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
